{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/playlist_features_filtered.csv', delimiter=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the playlist names \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "encoded_playlist_names = tokenizer(text = df.name.to_list(),        \n",
    "                           add_special_tokens=True,\n",
    "                           padding = 'max_length',\n",
    "                           truncation = 'longest_first',\n",
    "                           max_length = 300,\n",
    "                           return_attention_mask = True)\n",
    "\n",
    "input_ids = encoded_playlist_names['input_ids']\n",
    "attention_masks = encoded_playlist_names['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7588, 13)\n"
     ]
    }
   ],
   "source": [
    "# put data into numpy arrays\n",
    "names = np.array(input_ids)\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = df[\n",
    "        [\"acousticness\",\"danceability\",\"duration_ms\",\"energy\",\"instrumentalness\",\"key\",\"liveness\",\"loudness\",\"mode\",\"speechiness\",\"tempo\",\"time_signature\",\"valence\"]\n",
    "    ].to_numpy()\n",
    "\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the input\n",
    "test_size = 0.1\n",
    "seed = 42\n",
    "train_inputs, test_inputs, train_labels, test_labels = \\\n",
    "            train_test_split(input_ids, labels, test_size=test_size, \n",
    "                             random_state=seed)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks, \n",
    "                                        labels, test_size=test_size, \n",
    "                                        random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the label scores\n",
    "score_scaler = StandardScaler()\n",
    "score_scaler.fit(train_labels)\n",
    "\n",
    "train_labels = score_scaler.transform(train_labels)\n",
    "test_labels = score_scaler.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 13\n",
    "\n",
    "def create_dataloaders(inputs, masks, labels, batch_size):\n",
    "    input_tensor = torch.tensor(inputs)\n",
    "    mask_tensor = torch.tensor(masks)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    dataset = TensorDataset(input_tensor, mask_tensor, \n",
    "                            labels_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                            shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "train_dataloader = create_dataloaders(train_inputs, train_masks, \n",
    "                                      train_labels, batch_size)\n",
    "\n",
    "test_dataloader = create_dataloaders(test_inputs, test_masks, \n",
    "                                     test_labels, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "from BertRegressor import BertRegressor\n",
    "model = BertRegressor(drop_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "#setting up the training env\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"device: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateo\\opt\\SpotifyRecommender\\model\\env\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up model parameters\n",
    "\n",
    "#define the adam optimizer with a 5e-5 learning rate\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8)\n",
    "\n",
    "#number of epochs\n",
    "epochs = 5\n",
    "\n",
    "#total steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, scheduler, loss_function, epochs,\n",
    "          train_dataloader, device, clip_value = 2):\n",
    "    \n",
    "    for _ in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm(train_dataloader):\n",
    "            batch_inputs, batch_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "            loss = loss_function(outputs.squeeze(), batch_labels.squeeze().float())\n",
    "            loss.backward()\n",
    "            clip_grad_norm(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\mateo\\AppData\\Local\\Temp\\ipykernel_5800\\3591340347.py:17: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  clip_grad_norm(model.parameters(), clip_value)\n",
      "100%|██████████| 526/526 [03:08<00:00,  2.79it/s]\n",
      "100%|██████████| 526/526 [03:09<00:00,  2.78it/s]\n",
      "100%|██████████| 526/526 [03:09<00:00,  2.78it/s]\n",
      "100%|██████████| 526/526 [03:09<00:00,  2.78it/s]\n",
      "100%|██████████| 526/526 [03:09<00:00,  2.78it/s]\n",
      "100%|██████████| 5/5 [15:45<00:00, 189.06s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train(model, optimizer, scheduler, loss_fn, epochs, train_dataloader, device, clip_value= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bert_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
